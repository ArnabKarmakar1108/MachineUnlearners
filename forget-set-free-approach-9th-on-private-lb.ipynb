{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":56167,"databundleVersionId":6535361,"sourceType":"competition"}],"dockerImageVersionId":30554,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stochastic Unlearning with Knowledge Preserving Loss\n\nThis notebook describes the solution of 9th place on private leaderboard. It ranked 3rd place on public leaderboard.\nThe detailed explanation of our methods can be found in pdf below:\nhttps://www.dropbox.com/scl/fi/izmzhmj3ktqk3ze6rhjv6/Kaggle_Unlearning_Challenge_Solution.pdf?rlkey=9pqw47izw4nuuthanw3czu1gi&dl=0","metadata":{}},{"cell_type":"code","source":"import os\nimport subprocess\n\nimport pandas as pd\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision.models import resnet18\nfrom torch.utils.data import DataLoader, Dataset\n\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-14T06:15:21.416491Z","iopub.execute_input":"2023-09-14T06:15:21.416847Z","iopub.status.idle":"2023-09-14T06:15:21.423381Z","shell.execute_reply.started":"2023-09-14T06:15:21.416818Z","shell.execute_reply":"2023-09-14T06:15:21.422169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# It's really important to add an accelerator to your notebook, as otherwise the submission will fail.\n# We recomment using the P100 GPU rather than T4 as it's faster and will increase the chances of passing the time cut-off threshold.\n\nif DEVICE != 'cuda':\n    raise RuntimeError('Make sure you have added an accelerator to your notebook; the submission will fail otherwise!')","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:15:21.42896Z","iopub.execute_input":"2023-09-14T06:15:21.429416Z","iopub.status.idle":"2023-09-14T06:15:21.435334Z","shell.execute_reply.started":"2023-09-14T06:15:21.42939Z","shell.execute_reply":"2023-09-14T06:15:21.434403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Helper functions for loading the hidden dataset.\n\nclass RandomGaussianNoise(object):\n    def __init__(self, mean=0.0, std=1.0):\n        self.mean = mean\n        self.std = std\n        \n    def __call__(self, image):\n        return image + torch.randn(image.size()) * self.std + self.mean\n\ndef load_example(df_row):\n    image = torchvision.io.read_image(df_row['image_path'])\n    \n    result = {\n        'image': image,\n        'image_id': df_row['image_id'],\n        'age_group': df_row['age_group'],\n        'age': df_row['age'],\n        'person_id': df_row['person_id']\n    }\n    return result\n\n\nclass HiddenDataset(Dataset):\n    '''The hidden dataset.'''\n    def __init__(self, split='train'):\n        super().__init__()\n        self.examples = []\n        self.transform = None\n        if split == 'train' or split == 'retain' or split == 'forget':\n            from torchvision import transforms\n            self.transform = transforms.Compose([\n                RandomGaussianNoise(mean=0.0, std=0.1), # Add Data Augmentation (Random Gaussian Noise)\n            ])\n        df = pd.read_csv(f'/kaggle/input/neurips-2023-machine-unlearning/{split}.csv')\n        df['image_path'] = df['image_id'].apply(\n            lambda x: os.path.join('/kaggle/input/neurips-2023-machine-unlearning/', 'images', x.split('-')[0], x.split('-')[1] + '.png'))\n        df = df.sort_values(by='image_path')\n        df.apply(lambda row: self.examples.append(load_example(row)), axis=1)\n        if len(self.examples) == 0:\n            raise ValueError('No examples.')\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        example = self.examples[idx]\n        image = example['image']\n        image = image.to(torch.float32)\n        if self.transform is not None:\n            image = self.transform(image)\n        example['image'] = image\n        return example\n\n\ndef get_dataset(batch_size):\n    '''Get the dataset.'''\n    retain_ds = HiddenDataset(split='retain')\n    forget_ds = HiddenDataset(split='forget')\n    val_ds = HiddenDataset(split='validation')\n\n    retain_loader = DataLoader(retain_ds, batch_size=batch_size, shuffle=True)\n    forget_loader = DataLoader(forget_ds, batch_size=batch_size, shuffle=True)\n    validation_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n\n    return retain_loader, forget_loader, validation_loader","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:15:21.437644Z","iopub.execute_input":"2023-09-14T06:15:21.437967Z","iopub.status.idle":"2023-09-14T06:15:21.450776Z","shell.execute_reply.started":"2023-09-14T06:15:21.437935Z","shell.execute_reply":"2023-09-14T06:15:21.449671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unlearning(\n    net,\n    retain_loader,\n    forget_loader,\n    val_loader):\n    \"\"\"Simple unlearning by finetuning.\"\"\"\n    cycles = 4              # number of cycles\n    epochs = [2, 1, 1, 1]   # epochs for each cycle\n    num_init = [6, 6, 6, 6] # number of re-initializing parameters for each cycle, 1/10 of the model parameters (layer level)\n    \n    # Clone the original model with requires_grad=False\n    from copy import deepcopy\n    cloned_net = deepcopy(net)\n    for param in cloned_net.parameters():\n        param.requires_grad = False\n        \n    # Knowledge Preserving Loss (MSE loss) used in Remembering Phase\n    criterion = nn.MSELoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n    \n    # Custom lr scheduling\n    def adjust_lr(epoch):\n        # base_lr * lambda\n        # last additional element 0 for preventing indexing error at the last scheduler.step()\n        lambda_list = [0.5, 1, 1, 1, 1, 0] # make lr=[0.0005, 0.001, 0.001, 0.001, 0.001] for each epoch\n        return lambda_list[epoch]\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=adjust_lr)\n    net.train()\n\n    # Setting seleciton pool, which will be selected to be re-initialized in Forgetting Phase\n    params = [p for p in net.named_parameters() if 'downsample' not in p[0] and 'fc' not in p[0]] # do not re-initialize FC and Downsample\n\n    import random\n    for cycle in range(cycles):\n        ### Forgetting Phase: Stochastic Re-initialization ###\n        # Randomly reinitialize part of the network parameters (layer level)\n        with torch.no_grad():\n            for _ in range(num_init[cycle]):\n                name, param = random.choice(params) # Sampling with replacement\n                if 'weight' in name:\n                    nn.init.normal_(param, mean=0, std=0.01) # Using normal distribution for initialization\n                elif 'bias' in name:\n                    nn.init.zeros_(param) # Initializing biases to zero\n\n        ### Remembering Phase: Knowledge Preserving Loss ###\n        # Remind the model about retain set\n        # Only retain set is utilized\n        for ep in range(epochs[cycle]):\n            for i, sample in enumerate(retain_loader):\n                inputs = sample[\"image\"]\n                inputs = inputs.to(DEVICE)\n            \n                optimizer.zero_grad()\n                targets = cloned_net(inputs)\n                outputs = net(inputs)\n                loss = criterion(outputs, targets) # Calculate loss between original model output and unlearned model output\n                loss.backward()\n                optimizer.step()\n            scheduler.step()\n                \n    net.eval()","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:15:21.45217Z","iopub.execute_input":"2023-09-14T06:15:21.452595Z","iopub.status.idle":"2023-09-14T06:15:21.463494Z","shell.execute_reply.started":"2023-09-14T06:15:21.452561Z","shell.execute_reply":"2023-09-14T06:15:21.462576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists('/kaggle/input/neurips-2023-machine-unlearning/empty.txt'):\n    # mock submission\n    subprocess.run('touch submission.zip', shell=True)\nelse:\n    \n    # Note: it's really important to create the unlearned checkpoints outside of the working directory \n    # as otherwise this notebook may fail due to running out of disk space.\n    # The below code saves them in /kaggle/tmp to avoid that issue.\n    \n    os.makedirs('/kaggle/tmp', exist_ok=True)\n    retain_loader, forget_loader, validation_loader = get_dataset(64)\n    net = resnet18(weights=None, num_classes=10)\n    net.to(DEVICE)\n    for i in range(512):\n        net.load_state_dict(torch.load('/kaggle/input/neurips-2023-machine-unlearning/original_model.pth'))\n        unlearning(net, retain_loader, forget_loader, validation_loader)\n        state = net.state_dict()\n        torch.save(state, f'/kaggle/tmp/unlearned_checkpoint_{i}.pth')\n        \n    # Ensure that submission.zip will contain exactly 512 checkpoints \n    # (if this is not the case, an exception will be thrown).\n    unlearned_ckpts = os.listdir('/kaggle/tmp')\n    if len(unlearned_ckpts) != 512:\n        raise RuntimeError('Expected exactly 512 checkpoints. The submission will throw an exception otherwise.')\n        \n    subprocess.run('zip submission.zip /kaggle/tmp/*.pth', shell=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-14T06:15:21.466893Z","iopub.execute_input":"2023-09-14T06:15:21.467441Z","iopub.status.idle":"2023-09-14T06:15:21.4832Z","shell.execute_reply.started":"2023-09-14T06:15:21.467409Z","shell.execute_reply":"2023-09-14T06:15:21.482336Z"},"trusted":true},"execution_count":null,"outputs":[]}]}